{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign2Text: Sign Language to Text Translation Training\n",
    "\n",
    "This notebook guides you through the process of training a deep learning model to translate sign language videos to text. We'll be using a hybrid CNN-LSTM architecture that can optionally incorporate hand pose features.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Setup**: Import dependencies and configure environment\n",
    "2. **Data Preparation**: Load and explore the sign language dataset\n",
    "3. **Dataset & DataLoader**: Create PyTorch datasets and dataloaders\n",
    "4. **Model Architecture**: Build and explore the CNN-LSTM model\n",
    "5. **Training Configuration**: Configure hyperparameters and optimization\n",
    "6. **Training Loop**: Train the model with monitoring and evaluation\n",
    "7. **Evaluation**: Analyze model performance\n",
    "8. **Export**: Save the model for inference\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary libraries and modules, and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Import project modules\n",
    "from ml.models.cnn_lstm import create_model, CNNLSTM, CNNLSTMWithHandPose\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Configuration Parameters\n",
    "\n",
    "Here, we define the configuration parameters for our training process. These include paths to data, model parameters, and hyperparameters for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directories\n",
    "DATA_DIR = \"../../processed_data\"\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train_set.csv\")\n",
    "VAL_CSV = os.path.join(DATA_DIR, \"val_set.csv\")\n",
    "TEST_CSV = os.path.join(DATA_DIR, \"test_set.csv\")\n",
    "\n",
    "# Create output directory for saving models and logs\n",
    "OUTPUT_DIR = f\"../../models/cnn_lstm_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Model and training configuration\n",
    "config = {\n",
    "    \"batch_size\": 16,              # Smaller batch size for interactive exploration\n",
    "    \"num_epochs\": 30,              # Number of training epochs\n",
    "    \"learning_rate\": 0.001,        # Initial learning rate\n",
    "    \"weight_decay\": 1e-4,          # Weight decay (L2 regularization)\n",
    "    \"use_landmarks\": True,         # Whether to use hand landmark features\n",
    "    \"early_stopping\": 10,          # Number of epochs to wait before early stopping\n",
    "    \"max_seq_len\": 60,             # Maximum sequence length\n",
    "    \"model_params\": {\n",
    "        \"frame_dim\": (3, 224, 224),  # Input frame dimensions (C, H, W)\n",
    "        \"lstm_hidden_dim\": 512,      # LSTM hidden dimension\n",
    "        \"lstm_layers\": 2,            # Number of LSTM layers\n",
    "        \"dropout\": 0.5,              # Dropout probability\n",
    "        \"bidirectional\": True,       # Whether to use bidirectional LSTM\n",
    "        \"attention\": True            # Whether to use attention mechanism\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration to file\n",
    "with open(os.path.join(OUTPUT_DIR, \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "print(f\"Configuration saved to {os.path.join(OUTPUT_DIR, 'config.json')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Now, let's explore our preprocessed data. We'll load the CSV files that contain information about our training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_explore_data(csv_path):\n",
    "    \"\"\"Load and explore the dataset from a CSV file.\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Warning: {csv_path} does not exist. Please preprocess data first.\")\n",
    "        return None\n",
    "    \n",
    "    # Load the dataset CSV\n",
    "    data_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Display basic information\n",
    "    print(f\"Dataset contains {len(data_df)} samples\")\n",
    "    print(f\"Number of unique classes: {len(data_df['label'].unique())}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    display(data_df.head())\n",
    "    \n",
    "    # Display class distribution\n",
    "    class_dist = data_df['label'].value_counts()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    class_dist.plot(kind='bar')\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display feature paths distribution\n",
    "    print(f\"Sample feature path: {data_df['feature_path'].iloc[0]}\")\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "# Explore training set\n",
    "print(\"Exploring training set:\")\n",
    "train_df = load_and_explore_data(TRAIN_CSV)\n",
    "\n",
    "# Explore validation set\n",
    "print(\"\\nExploring validation set:\")\n",
    "val_df = load_and_explore_data(VAL_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring a Sample\n",
    "\n",
    "Let's examine an individual sample from our dataset to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_sample(data_dir, feature_path):\n",
    "    \"\"\"Explore a single sample from the dataset.\"\"\"\n",
    "    full_path = os.path.join(data_dir, feature_path)\n",
    "    \n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"Warning: {full_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Load the sample\n",
    "    data = np.load(full_path, allow_pickle=True)\n",
    "    \n",
    "    # Print available keys\n",
    "    print(f\"Available keys: {list(data.keys())}\")\n",
    "    \n",
    "    # Explore frames\n",
    "    frames = data['frames']\n",
    "    print(f\"Frames shape: {frames.shape}\")\n",
    "    print(f\"Number of frames: {frames.shape[0]}\")\n",
    "    print(f\"Frame dimensions: {frames.shape[1:]}\")\n",
    "    \n",
    "    # Visualize a few frames\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    frame_indices = [0, min(5, frames.shape[0]-1), min(10, frames.shape[0]-1), min(frames.shape[0]-1, 15)]\n",
    "    \n",
    "    for i, idx in enumerate(frame_indices):\n",
    "        if idx < frames.shape[0]:\n",
    "            axes[i].imshow(frames[idx])\n",
    "            axes[i].set_title(f\"Frame {idx}\")\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Explore hand landmarks if available\n",
    "    if 'hand_landmarks' in data:\n",
    "        hand_landmarks = data['hand_landmarks']\n",
    "        print(f\"\\nHand landmarks shape: {len(hand_landmarks)} frames of landmarks\")\n",
    "        \n",
    "        # Show example of landmark data structure\n",
    "        if len(hand_landmarks) > 0:\n",
    "            landmark_frame = hand_landmarks[0]\n",
    "            print(f\"Landmark keys for a frame: {list(landmark_frame.keys())}\")\n",
    "            \n",
    "            # Check if landmarks are available for left and right hands\n",
    "            if landmark_frame['left'] is not None:\n",
    "                print(f\"Left hand landmarks shape: {landmark_frame['left'].shape}\")\n",
    "            else:\n",
    "                print(\"Left hand landmarks not detected in this frame\")\n",
    "                \n",
    "            if landmark_frame['right'] is not None:\n",
    "                print(f\"Right hand landmarks shape: {landmark_frame['right'].shape}\")\n",
    "            else:\n",
    "                print(\"Right hand landmarks not detected in this frame\")\n",
    "\n",
    "# Explore a sample from the training set\n",
    "if train_df is not None and len(train_df) > 0:\n",
    "    sample_feature_path = train_df['feature_path'].iloc[0]\n",
    "    print(f\"Exploring sample: {sample_feature_path}\")\n",
    "    explore_sample(DATA_DIR, sample_feature_path)\n",
    "else:\n",
    "    print(\"Cannot explore sample: training data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader\n",
    "\n",
    "Now, let's define our custom PyTorch Dataset class for handling sign language video data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignVideoDataset(Dataset):\n",
    "    \"\"\"Dataset for loading sign language video data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_csv,\n",
    "        root_dir,\n",
    "        transform=None,\n",
    "        use_landmarks=True,\n",
    "        max_seq_len=60\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            data_csv: Path to the CSV file with annotations\n",
    "            root_dir: Directory containing the data files\n",
    "            transform: Optional transform to apply to frames\n",
    "            use_landmarks: Whether to load landmark data\n",
    "            max_seq_len: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.data_df = pd.read_csv(data_csv)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.use_landmarks = use_landmarks\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Create label to index mapping\n",
    "        self.classes = sorted(self.data_df['label'].unique())\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        print(f\"Initialized dataset with {len(self.data_df)} samples and {len(self.classes)} classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and return a sample from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing data for the requested sample\n",
    "        \"\"\"\n",
    "        sample_info = self.data_df.iloc[idx]\n",
    "\n",
    "        # Load preprocessed data\n",
    "        feature_path = os.path.join(self.root_dir, sample_info['feature_path'])\n",
    "        data = np.load(feature_path, allow_pickle=True)\n",
    "\n",
    "        # Get frames and pad/truncate if needed\n",
    "        frames = data['frames']\n",
    "        seq_len = min(frames.shape[0], self.max_seq_len)\n",
    "\n",
    "        # Create padded array\n",
    "        padded_frames = np.zeros((self.max_seq_len, *frames.shape[1:]), dtype=frames.dtype)\n",
    "        padded_frames[:seq_len] = frames[:seq_len]\n",
    "\n",
    "        # Convert to tensor and permute to (seq_len, channels, height, width)\n",
    "        frames_tensor = torch.FloatTensor(padded_frames).permute(0, 3, 1, 2)\n",
    "\n",
    "        # Apply transform if available\n",
    "        if self.transform:\n",
    "            # Apply transform to each frame\n",
    "            transformed_frames = []\n",
    "            for i in range(seq_len):\n",
    "                transformed_frames.append(self.transform(frames_tensor[i]))\n",
    "\n",
    "            # Pad with zeros\n",
    "            for i in range(seq_len, self.max_seq_len):\n",
    "                transformed_frames.append(torch.zeros_like(transformed_frames[0]))\n",
    "\n",
    "            frames_tensor = torch.stack(transformed_frames)\n",
    "\n",
    "        # Load landmarks if needed\n",
    "        landmarks_tensor = None\n",
    "        if self.use_landmarks and 'hand_landmarks' in data:\n",
    "            hand_landmarks = data['hand_landmarks']\n",
    "\n",
    "            # Process and pad landmarks\n",
    "            landmarks_list = []\n",
    "            for i in range(min(len(hand_landmarks), self.max_seq_len)):\n",
    "                # Extract left and right hand landmarks\n",
    "                left = hand_landmarks[i]['left']\n",
    "                right = hand_landmarks[i]['right']\n",
    "\n",
    "                # Flatten and concatenate\n",
    "                if left is not None and right is not None:\n",
    "                    # Both hands visible\n",
    "                    combined = np.concatenate([left.flatten(), right.flatten()])\n",
    "                elif left is not None:\n",
    "                    # Only left hand visible\n",
    "                    combined = np.concatenate([left.flatten(), np.zeros_like(left.flatten())])\n",
    "                elif right is not None:\n",
    "                    # Only right hand visible\n",
    "                    combined = np.concatenate([np.zeros(63), right.flatten()])\n",
    "                else:\n",
    "                    # No hands visible\n",
    "                    combined = np.zeros(126)\n",
    "\n",
    "                landmarks_list.append(combined)\n",
    "\n",
    "            # Pad if needed\n",
    "            while len(landmarks_list) < self.max_seq_len:\n",
    "                landmarks_list.append(np.zeros(126))\n",
    "\n",
    "            landmarks_tensor = torch.FloatTensor(np.array(landmarks_list))\n",
    "\n",
    "        # Get label\n",
    "        label = self.class_to_idx[sample_info['label']]\n",
    "        label_tensor = torch.LongTensor([label])[0]\n",
    "\n",
    "        # Create output dictionary\n",
    "        output = {\n",
    "            'frames': frames_tensor,\n",
    "            'label': label_tensor,\n",
    "            'seq_len': torch.LongTensor([seq_len])[0]\n",
    "        }\n",
    "\n",
    "        if landmarks_tensor is not None:\n",
    "            output['landmarks'] = landmarks_tensor\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets and DataLoaders\n",
    "\n",
    "Now, let's create our datasets and dataloaders for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "print(\"Creating training dataset...\")\n",
    "train_dataset = SignVideoDataset(\n",
    "    TRAIN_CSV,\n",
    "    DATA_DIR,\n",
    "    use_landmarks=config[\"use_landmarks\"],\n",
    "    max_seq_len=config[\"max_seq_len\"]\n",
    ")\n",
    "\n",
    "print(\"\\nCreating validation dataset...\")\n",
    "val_dataset = SignVideoDataset(\n",
    "    VAL_CSV,\n",
    "    DATA_DIR,\n",
    "    use_landmarks=config[\"use_landmarks\"],\n",
    "    max_seq_len=config[\"max_seq_len\"]\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=2  # Adjust based on your system\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=2  # Adjust based on your system\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine a Batch of Data\n",
    "\n",
    "Let's examine a batch of data to ensure everything is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_batch(dataloader):\n",
    "    \"\"\"Examine a batch from a dataloader.\"\"\"\n",
    "    # Get a batch of data\n",
    "    batch = next(iter(dataloader))\n",
    "    \n",
    "    # Print batch shapes\n",
    "    print(\"Batch contents:\")\n",
    "    for key, value in batch.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(value)}\")\n",
    "    \n",
    "    # Visualize a few frames from the first sample\n",
    "    frames = batch['frames'][0]  # First sample in batch\n",
    "    seq_len = batch['seq_len'][0].item()\n",
    "    \n",
    "    # Display 4 frames from the sequence\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    frame_indices = [0, min(seq_len//3, seq_len-1), min(2*seq_len//3, seq_len-1), min(seq_len-1, seq_len-1)]\n",
    "    \n",
    "    for i, idx in enumerate(frame_indices):\n",
    "        # Convert from (C, H, W) to (H, W, C) and normalize for display\n",
    "        frame = frames[idx].permute(1, 2, 0).numpy()\n",
    "        # Normalize to [0, 1] range if needed\n",
    "        if frame.max() > 1.0:\n",
    "            frame = frame / 255.0\n",
    "        axes[i].imshow(frame)\n",
    "        axes[i].set_title(f\"Frame {idx}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # If landmarks are available, visualize them too\n",
    "    if 'landmarks' in batch:\n",
    "        landmarks = batch['landmarks'][0]  # First sample in batch\n",
    "        \n",
    "        print(f\"\\nLandmarks shape: {landmarks.shape}\")\n",
    "        \n",
    "        # Plot the sum of landmark positions over time as a simple visualization\n",
    "        landmark_activity = torch.sum(torch.abs(landmarks), dim=1).numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(landmark_activity)\n",
    "        plt.title('Hand Landmark Activity Over Time')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Total Landmark Activity')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Examine a batch from the training dataloader\n",
    "print(\"Examining a batch from the training dataloader:\")\n",
    "examine_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "Now let's look at our model architecture. We'll be using a CNN-LSTM model, optionally with hand pose features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the model type based on config\n",
    "model_name = \"cnn_lstm_handpose\" if config[\"use_landmarks\"] else \"cnn_lstm\"\n",
    "\n",
    "# Create the model\n",
    "num_classes = len(train_dataset.classes)\n",
    "model = create_model(\n",
    "    model_name,\n",
    "    num_classes=num_classes,\n",
    "    **config[\"model_params\"]\n",
    ")\n",
    "\n",
    "# Print model architecture\n",
    "print(f\"Model type: {model_name}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"\\nModel architecture:\\n{model}\")\n",
    "\n",
    "# Print number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {num_params:,}\")\n",
    "print(f\"Trainable parameters: {num_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass Test\n",
    "\n",
    "Let's test the forward pass through our model with a sample batch to ensure everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of data\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Move batch to device\n",
    "frames = batch['frames'].to(device)\n",
    "labels = batch['label'].to(device)\n",
    "\n",
    "# Print input shapes\n",
    "print(f\"Input frames shape: {frames.shape}\")\n",
    "print(f\"Input labels shape: {labels.shape}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        if config[\"use_landmarks\"]:\n",
    "            landmarks = batch['landmarks'].to(device)\n",
    "            print(f\"Input landmarks shape: {landmarks.shape}\")\n",
    "            outputs = model(frames, landmarks)\n",
    "        else:\n",
    "            outputs = model(frames)\n",
    "        \n",
    "        # Print output shapes\n",
    "        if isinstance(outputs, dict):\n",
    "            for key, value in outputs.items():\n",
    "                print(f\"Output {key} shape: {value.shape}\")\n",
    "        else:\n",
    "            print(f\"Output shape: {outputs.shape}\")\n",
    "        \n",
    "        print(\"\\n✓ Forward pass successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during forward pass: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "Now, let's set up our training configuration, including the loss function, optimizer, and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(f\"Loss function: {criterion}\")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config[\"learning_rate\"],\n",
    "    weight_decay=config[\"weight_decay\"]\n",
    ")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "print(f\"LR Scheduler: {scheduler}\")\n",
    "\n",
    "# Define TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=os.path.join(OUTPUT_DIR, 'logs'))\n",
    "print(f\"TensorBoard logs will be saved to {os.path.join(OUTPUT_DIR, 'logs')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "Now, let's define our training and validation functions, and implement the main training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with tqdm(train_loader, desc=\"Training\") as pbar:\n",
    "        for batch in pbar:\n",
    "            # Move batch to device\n",
    "            frames = batch['frames'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Handle different model input types\n",
    "            if 'landmarks' in batch and hasattr(model, 'forward') and 'landmarks' in model.forward.__code__.co_varnames:\n",
    "                landmarks = batch['landmarks'].to(device)\n",
    "                outputs = model(frames, landmarks)\n",
    "            else:\n",
    "                outputs = model(frames)\n",
    "\n",
    "            # Get logits\n",
    "            if isinstance(outputs, dict):\n",
    "                logits = outputs['logits']\n",
    "            else:\n",
    "                logits = outputs\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'acc': f\"{100 * correct / total:.2f}%\"\n",
    "            })\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model on the validation set.\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(val_loader, desc=\"Validation\") as pbar:\n",
    "            for batch in pbar:\n",
    "                # Move batch to device\n",
    "                frames = batch['frames'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                if 'landmarks' in batch and hasattr(model, 'forward') and 'landmarks' in model.forward.__code__.co_varnames:\n",
    "                    landmarks = batch['landmarks'].to(device)\n",
    "                    outputs = model(frames, landmarks)\n",
    "                else:\n",
    "                    outputs = model(frames)\n",
    "\n",
    "                # Get logits\n",
    "                if isinstance(outputs, dict):\n",
    "                    logits = outputs['logits']\n",
    "                else:\n",
    "                    logits = outputs\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                # Update metrics\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Store predictions and labels for confusion matrix\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f\"{loss.item():.4f}\",\n",
    "                    'acc': f\"{100 * correct / total:.2f}%\"\n",
    "                })\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return avg_loss, accuracy, all_preds, all_labels\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, val_loss, val_acc, output_dir, is_best=False):\n",
    "    \"\"\"Save a checkpoint of the model.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc\n",
    "    }\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "    \n",
    "    # Save latest checkpoint\n",
    "    checkpoint_path = os.path.join(output_dir, 'checkpoint_latest.pth')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    # Save best model\n",
    "    if is_best:\n",
    "        best_path = os.path.join(output_dir, 'model_best.pth')\n",
    "        torch.save(checkpoint, best_path)\n",
    "        \n",
    "        # Save model architecture as text\n",
    "        with open(os.path.join(output_dir, 'model_architecture.txt'), 'w') as f:\n",
    "            f.write(str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop\n",
    "\n",
    "Now let's implement the main training loop that will train our model for the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = config['num_epochs']\n",
    "early_stopping = config['early_stopping']\n",
    "\n",
    "# Initialize tracking variables\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "epochs_without_improvement = 0\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs\")\n",
    "print(f\"Early stopping patience: {early_stopping} epochs\")\n",
    "print(f\"Outputs will be saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Train for one epoch\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate if scheduler is available\n",
    "    if scheduler is not None:\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "    \n",
    "    # Log metrics\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Log epoch summary\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed in {epoch_time:.2f}s\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Check if this is the best model\n",
    "    is_best = False\n",
    "    if val_acc > best_val_acc:\n",
    "        print(f\"Validation accuracy improved from {best_val_acc:.4f} to {val_acc:.4f}\")\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        is_best = True\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"No improvement for {epochs_without_improvement} epochs\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\n",
    "        model, optimizer, scheduler, epoch,\n",
    "        val_loss, val_acc, OUTPUT_DIR, is_best\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping > 0 and epochs_without_improvement >= early_stopping:\n",
    "        print(f\"Early stopping after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "# Close tensorboard writer\n",
    "writer.close()\n",
    "\n",
    "# Log training summary\n",
    "print(f\"\\nTraining completed. Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Save training history\n",
    "history_path = os.path.join(OUTPUT_DIR, 'training_history.json')\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump({k: [float(v) for v in vals] for k, vals in history.items()}, f, indent=4)\n",
    "    \n",
    "print(f\"Training history saved to {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation and Visualization\n",
    "\n",
    "Now, let's visualize our training results and evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot the training history.\"\"\"\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    ax1.plot(history['train_loss'], label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot training and validation accuracy\n",
    "    ax2.plot(history['train_acc'], label='Train Accuracy')\n",
    "    ax2.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Also save the figure to file\n",
    "    fig.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "    print(f\"Training history plot saved to {os.path.join(OUTPUT_DIR, 'training_history.png')}\")\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Let's evaluate the model on the validation set and create a confusion matrix to visualize performance across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"Evaluating model on validation set...\")\n",
    "val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, criterion, device)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\\n\")\n",
    "\n",
    "# Get class names\n",
    "class_names = val_dataset.classes\n",
    "\n",
    "# If there are too many classes, sample a subset for visualization\n",
    "max_classes_to_show = 20\n",
    "if len(class_names) > max_classes_to_show:\n",
    "    # Sample classes for visualization\n",
    "    unique_labels = np.unique(val_labels)\n",
    "    if len(unique_labels) > max_classes_to_show:\n",
    "        selected_classes = np.random.choice(unique_labels, max_classes_to_show, replace=False)\n",
    "        mask = np.isin(val_labels, selected_classes)\n",
    "        val_preds_subset = np.array(val_preds)[mask]\n",
    "        val_labels_subset = np.array(val_labels)[mask]\n",
    "        class_indices = selected_classes\n",
    "    else:\n",
    "        val_preds_subset = val_preds\n",
    "        val_labels_subset = val_labels\n",
    "        class_indices = unique_labels\n",
    "    \n",
    "    # Get class names for the selected indices\n",
    "    class_names_subset = [class_names[i] for i in class_indices]\n",
    "else:\n",
    "    val_preds_subset = val_preds\n",
    "    val_labels_subset = val_labels\n",
    "    class_names_subset = class_names\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(val_labels_subset, val_preds_subset)\n",
    "\n",
    "# Normalize confusion matrix\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', \n",
    "            xticklabels=class_names_subset, yticklabels=class_names_subset)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix (Normalized)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=class_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference and Export\n",
    "\n",
    "Finally, let's test the model on a sample and export it for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(model, dataset, sample_idx, device):\n",
    "    \"\"\"Make a prediction for a single sample.\"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get the sample\n",
    "    sample = dataset[sample_idx]\n",
    "    frames = sample['frames'].unsqueeze(0).to(device)  # Add batch dimension\n",
    "    true_label = sample['label'].item()\n",
    "    true_class = dataset.classes[true_label]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        if 'landmarks' in sample and hasattr(model, 'forward') and 'landmarks' in model.forward.__code__.co_varnames:\n",
    "            landmarks = sample['landmarks'].unsqueeze(0).to(device)\n",
    "            outputs = model(frames, landmarks)\n",
    "        else:\n",
    "            outputs = model(frames)\n",
    "        \n",
    "        # Get logits\n",
    "        if isinstance(outputs, dict):\n",
    "            logits = outputs['logits']\n",
    "            attention = outputs.get('attention_weights', None)\n",
    "        else:\n",
    "            logits = outputs\n",
    "            attention = None\n",
    "    \n",
    "    # Get predicted class\n",
    "    _, predicted_idx = torch.max(logits, 1)\n",
    "    predicted_label = predicted_idx.item()\n",
    "    predicted_class = dataset.classes[predicted_label]\n",
    "    \n",
    "    # Get probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)[0]\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"True class: {true_class} (idx: {true_label})\")\n",
    "    print(f\"Predicted class: {predicted_class} (idx: {predicted_label})\")\n",
    "    print(f\"Confidence: {probs[predicted_label]:.4f}\")\n",
    "    \n",
    "    # Visualize frames with attention if available\n",
    "    frames_np = frames[0].cpu().numpy()\n",
    "    seq_len = sample['seq_len'].item()\n",
    "    \n",
    "    # Select frames to visualize\n",
    "    num_frames = min(8, seq_len)\n",
    "    frame_indices = np.linspace(0, seq_len-1, num_frames, dtype=int)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, num_frames, figsize=(16, 4))\n",
    "    \n",
    "    for i, idx in enumerate(frame_indices):\n",
    "        # Display frame\n",
    "        frame = frames_np[idx].transpose(1, 2, 0)  # C,H,W -> H,W,C\n",
    "        # Normalize if needed\n",
    "        if frame.max() > 1.0:\n",
    "            frame = frame / 255.0\n",
    "        axes[i].imshow(frame)\n",
    "        \n",
    "        # Add attention weight as title if available\n",
    "        if attention is not None:\n",
    "            att_weight = attention[0, idx].item()\n",
    "            axes[i].set_title(f\"Att: {att_weight:.3f}\")\n",
    "        else:\n",
    "            axes[i].set_title(f\"Frame {idx}\")\n",
    "        \n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"True: {true_class}, Pred: {predicted_class} ({probs[predicted_label]:.4f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'true_label': true_label,\n",
    "        'true_class': true_class,\n",
    "        'predicted_label': predicted_label,\n",
    "        'predicted_class': predicted_class,\n",
    "        'confidence': probs[predicted_label].item(),\n",
    "        'probabilities': probs.cpu().numpy()\n",
    "    }\n",
    "\n",
    "# Test the model on a few random samples from the validation set\n",
    "print(\"Testing model on random samples from validation set:\")\n",
    "num_samples = 3\n",
    "for i in range(num_samples):\n",
    "    sample_idx = np.random.randint(0, len(val_dataset))\n",
    "    print(f\"\\nSample {i+1}/{num_samples} (index {sample_idx}):\")\n",
    "    predict_sample(model, val_dataset, sample_idx, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the Model\n",
    "\n",
    "Finally, let's export the model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = os.path.join(OUTPUT_DIR, 'model_final.pth')\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'class_names': val_dataset.classes,\n",
    "    'config': config\n",
    "}, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Export model in TorchScript format for deployment\n",
    "try:\n",
    "    # Prepare a sample input\n",
    "    sample = next(iter(val_loader))\n",
    "    sample_frames = sample['frames'].to(device)\n",
    "    \n",
    "    # Create scripted model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if config[\"use_landmarks\"]:\n",
    "            sample_landmarks = sample['landmarks'].to(device)\n",
    "            traced_model = torch.jit.trace(model, (sample_frames, sample_landmarks))\n",
    "        else:\n",
    "            traced_model = torch.jit.trace(model, sample_frames)\n",
    "    \n",
    "    # Save the scripted model\n",
    "    script_path = os.path.join(OUTPUT_DIR, 'model_scripted.pt')\n",
    "    traced_model.save(script_path)\n",
    "    print(f\"TorchScript model saved to {script_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exporting TorchScript model: {str(e)}\")\n",
    "    print(\"You may need to modify the model to make it exportable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've walked through the complete process of training a sign language recognition model using a CNN-LSTM architecture with optional hand pose features. Here's what we covered:\n",
    "\n",
    "1. **Setup and Data Preparation**: We configured our environment, loaded and explored the preprocessed sign language dataset.\n",
    "\n",
    "2. **Dataset & DataLoader**: We created a custom PyTorch dataset for handling sign language video data with optional hand landmarks.\n",
    "\n",
    "3. **Model Architecture**: We explored the CNN-LSTM model architecture, which combines CNN for spatial feature extraction and LSTM for temporal sequence processing.\n",
    "\n",
    "4. **Training**: We trained the model, monitoring its performance with TensorBoard and implementing early stopping to prevent overfitting.\n",
    "\n",
    "5. **Evaluation**: We evaluated the model's performance using accuracy metrics and confusion matrices to visualize class-wise performance.\n",
    "\n",
    "6. **Export**: Finally, we exported the model in both PyTorch and TorchScript formats for deployment.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further improve the model, you could:\n",
    "\n",
    "1. **Data Augmentation**: Implement techniques like random cropping, rotation, or temporal augmentation to increase the effective size of the training dataset.\n",
    "\n",
    "2. **Architecture Improvements**: Experiment with more sophisticated CNN backbones (ResNet, EfficientNet) or sequence models (Transformer).\n",
    "\n",
    "3. **Hyperparameter Tuning**: Use techniques like grid search or Bayesian optimization to find the best hyperparameters.\n",
    "\n",
    "4. **Transfer Learning**: Leverage pre-trained models on large-scale video datasets to improve feature extraction.\n",
    "\n",
    "5. **Deployment**: Deploy the model in a real-time application using the exported TorchScript model.\n",
    "\n",
    "Thank you for following along with this Sign2Text training notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
