{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Data Exploration Notebook\n",
    "\n",
    "This notebook explores sign language video data for the Sign2Text project. We'll load, visualize, and analyze the data to understand its characteristics before model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import HTML, display\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append('..')\n",
    "from data_processing.preprocess import SignVideoProcessor\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure Mediapipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "First, let's set up paths to our data and load it. The following code assumes you have a dataset of sign language videos, along with annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set paths to data\n",
    "# Update these paths to point to your actual data\n",
    "DATA_ROOT = \"../../../data\"  # Change this to your data directory\n",
    "VIDEO_DIR = os.path.join(DATA_ROOT, \"videos\")\n",
    "ANNOTATIONS_FILE = os.path.join(DATA_ROOT, \"annotations.csv\")\n",
    "\n",
    "# Check if paths exist\n",
    "print(f\"DATA_ROOT exists: {os.path.exists(DATA_ROOT)}\")\n",
    "print(f\"VIDEO_DIR exists: {os.path.exists(VIDEO_DIR)}\")\n",
    "print(f\"ANNOTATIONS_FILE exists: {os.path.exists(ANNOTATIONS_FILE)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load annotations if available\n",
    "annotations = None\n",
    "if os.path.exists(ANNOTATIONS_FILE):\n",
    "    annotations = pd.read_csv(ANNOTATIONS_FILE)\n",
    "    print(f\"Loaded {len(annotations)} annotations\")\n",
    "    display(annotations.head())\n",
    "else:\n",
    "    print(\"Annotations file not found. We'll create one from video files.\")\n",
    "    \n",
    "    # Create a list of video files\n",
    "    video_files = []\n",
    "    for ext in [\".mp4\", \".avi\", \".mov\"]:\n",
    "        video_files.extend(list(Path(VIDEO_DIR).glob(f\"**/*{ext}\")))\n",
    "    \n",
    "    # Create a dataframe with file information\n",
    "    annotations = pd.DataFrame({\n",
    "        \"filename\": [p.stem for p in video_files],\n",
    "        \"filepath\": [str(p) for p in video_files],\n",
    "        # Placeholder for labels - you'll need to add actual labels\n",
    "        \"label\": [\"unknown\" for _ in video_files]\n",
    "    })\n",
    "    \n",
    "    print(f\"Created annotations for {len(annotations)} video files\")\n",
    "    display(annotations.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Data Analysis\n",
    "\n",
    "Let's analyze the dataset to understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze the distribution of sign classes\n",
    "if 'label' in annotations.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    counts = annotations['label'].value_counts()\n",
    "    \n",
    "    # If there are many classes, show only top 30\n",
    "    if len(counts) > 30:\n",
    "        counts = counts.nlargest(30)\n",
    "        title = \"Distribution of Top 30 Sign Classes\"\n",
    "    else:\n",
    "        title = \"Distribution of Sign Classes\"\n",
    "        \n",
    "    sns.barplot(x=counts.index, y=counts.values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Sign Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Number of unique classes: {annotations['label'].nunique()}\")\n",
    "    print(f\"Most common class: {annotations['label'].value_counts().idxmax()} ({annotations['label'].value_counts().max()} instances)\")\n",
    "    print(f\"Least common class: {annotations['label'].value_counts().idxmin()} ({annotations['label'].value_counts().min()} instances)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze video properties\n",
    "def get_video_properties(video_path):\n",
    "    \"\"\"Extract basic properties of a video file.\"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return None\n",
    "        \n",
    "        # Extract properties\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = frame_count / fps if fps > 0 else 0\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        return {\n",
    "            'width': width,\n",
    "            'height': height,\n",
    "            'fps': fps,\n",
    "            'frame_count': frame_count,\n",
    "            'duration': duration\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {video_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Sample a subset of videos for analysis\n",
    "sample_size = min(100, len(annotations))  # Analyze up to 100 videos\n",
    "sample_videos = annotations.sample(sample_size, random_state=42)\n",
    "\n",
    "# Collect video properties\n",
    "video_properties = []\n",
    "for idx, row in tqdm(sample_videos.iterrows(), total=len(sample_videos), desc=\"Analyzing videos\"):\n",
    "    video_path = row['filepath'] if 'filepath' in row else os.path.join(VIDEO_DIR, row['filename'])\n",
    "    if not os.path.exists(video_path) and 'filepath' not in row:\n",
    "        # Try common extensions if filepath not specified\n",
    "        for ext in [\".mp4\", \".avi\", \".mov\"]:\n",
    "            test_path = os.path.join(VIDEO_DIR, f\"{row['filename']}{ext}\")\n",
    "            if os.path.exists(test_path):\n",
    "                video_path = test_path\n",
    "                break\n",
    "    \n",
    "    props = get_video_properties(video_path)\n",
    "    if props:\n",
    "        props['filename'] = row['filename']\n",
    "        if 'label' in row:\n",
    "            props['label'] = row['label']\n",
    "        video_properties.append(props)\n",
    "\n",
    "# Convert to DataFrame\n",
    "video_props_df = pd.DataFrame(video_properties)\n",
    "display(video_props_df.head())\n",
    "\n",
    "# Display summary statistics\n",
    "display(video_props_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize distributions of video properties\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Duration distribution\n",
    "sns.histplot(video_props_df['duration'], kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Video Duration Distribution (seconds)')\n",
    "axes[0, 0].set_xlabel('Duration (s)')\n",
    "\n",
    "# Frame count distribution\n",
    "sns.histplot(video_props_df['frame_count'], kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Frame Count Distribution')\n",
    "axes[0, 1].set_xlabel('Number of Frames')\n",
    "\n",
    "# Resolution distribution\n",
    "video_props_df['resolution'] = video_props_df['width'].astype(str) + 'x' + video_props_df['height'].astype(str)\n",
    "res_counts = video_props_df['resolution'].value_counts()\n",
    "sns.barplot(x=res_counts.index, y=res_counts.values, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Video Resolution Distribution')\n",
    "axes[1, 0].set_xlabel('Resolution')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# FPS distribution\n",
    "sns.histplot(video_props_df['fps'], kde=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('FPS Distribution')\n",
    "axes[1, 1].set_xlabel('Frames Per Second')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Video Visualization\n",
    "\n",
    "Let's visualize some sample videos from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def display_video_frames(video_path, max_frames=20, interval=2):\n",
    "    \"\"\"Display frames from a video.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Could not open video: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration = frame_count / fps if fps > 0 else 0\n",
    "    \n",
    "    print(f\"Video: {os.path.basename(video_path)}\")\n",
    "    print(f\"Duration: {duration:.2f} seconds, Frames: {frame_count}, FPS: {fps:.1f}\")\n",
    "    \n",
    "    # Calculate sampling interval to get desired number of frames\n",
    "    if frame_count > max_frames:\n",
    "        sample_interval = frame_count // max_frames\n",
    "    else:\n",
    "        sample_interval = 1\n",
    "    \n",
    "    frames = []\n",
    "    frame_idx = 0\n",
    "    \n",
    "    # Read frames\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        if frame_idx % sample_interval == 0:\n",
    "            # Convert from BGR to RGB for display\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame_rgb)\n",
    "            \n",
    "            if len(frames) >= max_frames:\n",
    "                break\n",
    "                \n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Display frames in a grid\n",
    "    rows = (len(frames) + 3) // 4  # 4 frames per row\n",
    "    fig, axes = plt.subplots(rows, min(4, len(frames)), figsize=(15, 3*rows))\n",
    "    \n",
    "    # Flatten axes array if needed\n",
    "    if rows == 1 and len(frames) < 4:\n",
    "        axes = [axes]\n",
    "    elif rows > 1:\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "    for i, frame in enumerate(frames):\n",
    "        if i < len(axes):\n",
    "            axes[i].imshow(frame)\n",
    "            axes[i].set_title(f\"Frame {i*sample_interval}\")\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(frames), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select a random video to visualize\n",
    "if len(video_properties) > 0:\n",
    "    random_video = video_props_df.sample(1).iloc[0]\n",
    "    video_path = os.path.join(VIDEO_DIR, random_video['filename'])\n",
    "    \n",
    "    # Try common extensions if needed\n",
    "    if not os.path.exists(video_path):\n",
    "        for ext in [\".mp4\", \".avi\", \".mov\"]:\n",
    "            test_path = os.path.join(VIDEO_DIR, f\"{random_video['filename']}{ext}\")\n",
    "            if os.path.exists(test_path):\n",
    "                video_path = test_path\n",
    "                break\n",
    "    \n",
    "    if os.path.exists(video_path):\n",
    "        display_video_frames(video_path)\n",
    "    else:\n",
    "        print(f\"Could not find video file for {random_video['filename']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hand Landmark Detection\n",
    "\n",
    "Let's explore hand landmark detection using MediaPipe, which is crucial for sign language recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_landmarks(video_path, max_frames=20):\n",
    "    \"\"\"Detect hand and pose landmarks in a video using MediaPipe.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Could not open video: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize MediaPipe Holistic\n",
    "    with mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Calculate sampling interval\n",
    "        if frame_count > max_frames:\n",
    "            sample_interval = frame_count // max_frames\n",
    "        else:\n",
    "            sample_interval = 1\n",
    "        \n",
    "        frames = []\n",
    "        landmarks = []\n",
    "        frame_idx = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            if frame_idx % sample_interval == 0:\n",
    "                # Convert to RGB for MediaPipe\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Process frame with MediaPipe\n",
    "                results = holistic.process(image)\n",
    "                \n",
    "                # Draw landmarks on the image\n",
    "                annotated_image = image.copy()\n",
    "                \n",
    "                # Draw pose landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    annotated_image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "                \n",
    "                # Draw left hand landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    annotated_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                \n",
    "                # Draw right hand landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    annotated_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                \n",
    "                frames.append(annotated_image)\n",
    "                landmarks.append({\n",
    "                    'pose': results.pose_landmarks,\n",
    "                    'left_hand': results.left_hand_landmarks,\n",
    "                    'right_hand': results.right_hand_landmarks\n",
    "                })\n",
    "                \n",
    "                if len(frames) >= max_frames:\n",
    "                    break\n",
    "            \n",
    "            frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Display frames in a grid\n",
    "    rows = (len(frames) + 3) // 4  # 4 frames per row\n",
    "    fig, axes = plt.subplots(rows, min(4, len(frames)), figsize=(15, 3*rows))\n",
    "    \n",
    "    # Flatten axes array if needed\n",
    "    if rows == 1 and len(frames) < 4:\n",
    "        axes = [axes]\n",
    "    elif rows > 1:\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "    for i, frame in enumerate(frames):\n",
    "        if i < len(axes):\n",
    "            axes[i].imshow(frame)\n",
    "            axes[i].set_title(f\"Frame {i*sample_interval}\")\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(frames), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Detect landmarks in a sample video\n",
    "if len(video_properties) > 0:\n",
    "    random_video = video_props_df.sample(1).iloc[0]\n",
    "    video_path = os.path.join(VIDEO_DIR, random_video['filename'])\n",
    "    \n",
    "    # Try common extensions if needed\n",
    "    if not os.path.exists(video_path):\n",
    "        for ext in [\".mp4\", \".avi\", \".mov\"]:\n",
    "            test_path = os.path.join(VIDEO_DIR, f\"{random_video['filename']}{ext}\")\n",
    "            if os.path.exists(test_path):\n",
    "                video_path = test_path\n",
    "                break\n",
    "    \n",
    "    if os.path.exists(video_path):\n",
    "        landmarks = detect_landmarks(video_path)\n",
    "    else:\n",
    "        print(f\"Could not find video file for {random_video['filename']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Landmark Analysis\n",
    "\n",
    "Let's analyze the detected landmarks for potential features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_hand_landmarks(landmarks):\n",
    "    \"\"\"Analyze hand landmarks from multiple frames.\"\"\"\n",
    "    if not landmarks:\n",
    "        return\n",
    "    \n",
    "    # Count frames with detected hands\n",
    "    left_hand_detected = sum(1 for l in landmarks if l['left_hand'] is not None)\n",
    "    right_hand_detected = sum(1 for l in landmarks if l['right_hand'] is not None)\n",
    "    both_hands_detected = sum(1 for l in landmarks \n",
    "                             if l['left_hand'] is not None and l['right_hand'] is not None)\n",
    "    \n",
    "    print(f\"Total frames analyzed: {len(landmarks)}\")\n",
    "    print(f\"Frames with left hand detected: {left_hand_detected} ({left_hand_detected/len(landmarks)*100:.1f}%)\")\n",
    "    print(f\"Frames with right hand detected: {right_hand_detected} ({right_hand_detected/len(landmarks)*100:.1f}%)\")\n",
    "    print(f\"Frames with both hands detected: {both_hands_detected} ({both_hands_detected/len(landmarks)*100:.1f}%)\")\n",
    "    \n",
    "    # Extract and plot hand movements\n",
    "    left_hand_positions = []\n",
    "    right_hand_positions = []\n",
    "    \n",
    "    for frame_landmarks in landmarks:\n",
    "        # Get left hand wrist position\n",
    "        if frame_landmarks['left_hand'] is not None:\n",
    "            wrist = frame_landmarks['left_hand'].landmark[0]  # Wrist is landmark 0\n",
    "            left_hand_positions.append((wrist.x, wrist.y, wrist.z))\n",
    "        else:\n",
    "            left_hand_positions.append((None, None, None))\n",
    "            \n",
    "        # Get right hand wrist position\n",
    "        if frame_landmarks['right_hand'] is not None:\n",
    "            wrist = frame_landmarks['right_hand'].landmark[0]  # Wrist is landmark 0\n",
    "            right_hand_positions.append((wrist.x, wrist.y, wrist.z))\n",
    "        else:\n",
    "            right_hand_positions.append((None, None, None))\n",
    "    \n",
    "    # Plot hand trajectories (2D)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Left hand\n",
    "    left_x = [p[0] for p in left_hand_positions if p[0] is not None]\n",
    "    left_y = [p[1] for p in left_hand_positions if p[1] is not None]\n",
    "    if left_x and left_y:\n",
    "        plt.plot(left_x, left_y, 'b-', alpha=0.7, label='Left Hand')\n",
    "        plt.scatter(left_x[0], left_y[0], color='blue', s=100, marker='o', label='Start')\n",
    "        plt.scatter(left_x[-1], left_y[-1], color='blue', s=100, marker='x', label='End')\n",
    "    \n",
    "    # Right hand\n",
    "    right_x = [p[0] for p in right_hand_positions if p[0] is not None]\n",
    "    right_y = [p[1] for p in right_hand_positions if p[1] is not None]\n",
    "    if right_x and right_y:\n",
    "        plt.plot(right_x, right_y, 'r-', alpha=0.7, label='Right Hand')\n",
    "        plt.scatter(right_x[0], right_y[0], color='red', s=100, marker='o')\n",
    "        plt.scatter(right_x[-1], right_y[-1], color='red', s=100, marker='x')\n",
    "    \n",
    "    plt.title('Hand Trajectory')\n",
    "    plt.xlabel('X Coordinate')\n",
    "    plt.ylabel('Y Coordinate')\n",
    "    plt.gca().invert_yaxis()  # Invert Y-axis to match image coordinates\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a 3D visualization if we have enough data points\n",
    "    left_x = [p[0] for p in left_hand_positions if p[0] is not None]\n",
    "    left_y = [p[1] for p in left_hand_positions if p[1] is not None]\n",
    "    left_z = [p[2] for p in left_hand_positions if p[2] is not None]\n",
    "    \n",
    "    right_x = [p[0] for p in right_hand_positions if p[0] is not None]\n",
    "    right_y = [p[1] for p in right_hand_positions if p[1] is not None]\n",
    "    right_z = [p[2] for p in right_hand_positions if p[2] is not None]\n",
    "    \n",
    "    if len(left_x) > 5 or len(right_x) > 5:\n",
    "        fig = plt.figure(figsize=(12, 10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        if len(left_x) > 5:\n",
    "            ax.plot(left_x, left_y, left_z, 'b-', alpha=0.7, label='Left Hand')\n",
    "            ax.scatter(left_x[0], left_y[0], left_z[0], color='blue', s=100, marker='o')\n",
    "            ax.scatter(left_x[-1], left_y[-1], left_z[-1], color='blue', s=100, marker='x')\n",
    "            \n",
    "        if len(right_x) > 5:\n",
    "            ax.plot(right_x, right_y, right_z, 'r-', alpha=0.7, label='Right Hand')\n",
    "            ax.scatter(right_x[0], right_y[0], right_z[0], color='red', s=100, marker='o')\n",
    "            ax.scatter(right_x[-1], right_y[-1], right_z[-1], color='red', s=100, marker='x')\n",
    "            \n",
    "        ax.set_title('3D Hand Trajectory')\n",
    "        ax.set_xlabel('X Coordinate')\n",
    "        ax.set_ylabel('Y Coordinate')\n",
    "        ax.set_zlabel('Z Coordinate')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'left_hand_positions': left_hand_positions,\n",
    "        'right_hand_positions': right_hand_positions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze landmarks if available\n",
    "if 'landmarks' in locals() and landmarks:\n",
    "    hand_movement = analyze_hand_landmarks(landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "\n",
    "Based on our analysis, let's explore potential features for sign language recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_hand_features(landmarks):\n",
    "    \"\"\"Extract useful features from hand landmarks.\"\"\"\n",
    "    if not landmarks:\n",
    "        return None\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for frame_idx, frame_landmarks in enumerate(landmarks):\n",
    "        frame_features = {\n",
    "            'frame_idx': frame_idx,\n",
    "            'left_hand_present': frame_landmarks['left_hand'] is not None,\n",
    "            'right_hand_present': frame_landmarks['right_hand'] is not None,\n",
    "        }\n",
    "        \n",
    "        # Left hand features\n",
    "        if frame_landmarks['left_hand'] is not None:\n",
    "            # Calculate hand centroid\n",
    "            x_sum = y_sum = z_sum = 0\n",
    "            for landmark in frame_landmarks['left_hand'].landmark:\n",
    "                x_sum += landmark.x\n",
    "                y_sum += landmark.y\n",
    "                z_sum += landmark.z\n",
    "            \n",
    "            num_landmarks = len(frame_landmarks['left_hand'].landmark)\n",
    "            frame_features['left_hand_centroid_x'] = x_sum / num_landmarks\n",
    "            frame_features['left_hand_centroid_y'] = y_sum / num_landmarks\n",
    "            frame_features['left_hand_centroid_z'] = z_sum / num_landmarks\n",
    "            \n",
    "            # Calculate spread of hand (variance as a measure of how open the hand is)\n",
    "            x_vars = sum((landmark.x - frame_features['left_hand_centroid_x'])**2 \n",
    "                        for landmark in frame_landmarks['left_hand'].landmark) / num_landmarks\n",
    "            y_vars = sum((landmark.y - frame_features['left_hand_centroid_y'])**2 \n",
    "                        for landmark in frame_landmarks['left_hand'].landmark) / num_landmarks\n",
    "            \n",
    "            frame_features['left_hand_spread'] = (x_vars + y_vars) ** 0.5\n",
    "        else:\n",
    "            # Set default values when hand not present\n",
    "            frame_features['left_hand_centroid_x'] = None\n",
    "            frame_features['left_hand_centroid_y'] = None\n",
    "            frame_features['left_hand_centroid_z'] = None\n",
    "            frame_features['left_hand_spread'] = None\n",
    "        \n",
    "        # Right hand features (similar to left hand)\n",
    "        if frame_landmarks['right_hand'] is not None:\n",
    "            # Calculate hand centroid\n",
    "            x_sum = y_sum = z_sum = 0\n",
    "            for landmark in frame_landmarks['right_hand'].landmark:\n",
    "                x_sum += landmark.x\n",
    "                y_sum += landmark.y\n",
    "                z_sum += landmark.z\n",
    "            \n",
    "            num_landmarks = len(frame_landmarks['right_hand'].landmark)\n",
    "            frame_features['right_hand_centroid_x'] = x_sum / num_landmarks\n",
    "            frame_features['right_hand_centroid_y'] = y_sum / num_landmarks\n",
    "            frame_features['right_hand_centroid_z'] = z_sum / num_landmarks\n",
    "            \n",
    "            # Calculate spread of hand\n",
    "            x_vars = sum((landmark.x - frame_features['right_hand_centroid_x'])**2 \n",
    "                        for landmark in frame_landmarks['right_hand'].landmark) / num_landmarks\n",
    "            y_vars = sum((landmark.y - frame_features['right_hand_centroid_y'])**2 \n",
    "                        for landmark in frame_landmarks['right_hand'].landmark) / num_landmarks\n",
    "            \n",
    "            frame_features['right_hand_spread'] = (x_vars + y_vars) ** 0.5\n",
    "        else:\n",
    "            # Set default values when hand not present\n",
    "            frame_features['right_hand_centroid_x'] = None\n",
    "            frame_features['right_hand_centroid_y'] = None\n",
    "            frame_features['right_hand_centroid_z'] = None\n",
    "            frame_features['right_hand_spread'] = None\n",
    "            \n",
    "        # Hand distance feature (when both hands are present)\n",
    "        if frame_features['left_hand_present'] and frame_features['right_hand_present']:\n",
    "            dx = frame_features['left_hand_centroid_x'] - frame_features['right_hand_centroid_x']\n",
    "            dy = frame_features['left_hand_centroid_y'] - frame_features['right_hand_centroid_y']\n",
    "            dz = frame_features['left_hand_centroid_z'] - frame_features['right_hand_centroid_z']\n",
    "            frame_features['hand_distance'] = (dx**2 + dy**2 + dz**2)**0.5\n",
    "        else:\n",
    "            frame_features['hand_distance'] = None\n",
    "        \n",
    "        features.append(frame_features)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract features if landmarks are available\n",
    "if 'landmarks' in locals() and landmarks:\n",
    "    features_df = extract_hand_features(landmarks)\n",
    "    display(features_df.head())\n",
    "    \n",
    "    # Visualize features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create subplot for hand presence\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(features_df['frame_idx'], features_df['left_hand_present'], 'b-', label='Left Hand')\n",
    "    plt.plot(features_df['frame_idx'], features_df['right_hand_present'], 'r-', label='Right Hand')\n",
    "    plt.title('Hand Presence')\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Present')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Create subplot for hand spread\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(features_df['frame_idx'], features_df['left_hand_spread'], 'b-', label='Left Hand Spread')\n",
    "    plt.plot(features_df['frame_idx'], features_df['right_hand_spread'], 'r-', label='Right Hand Spread')\n",
    "    plt.title('Hand Spread (measure of how open the hand is)')\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Spread')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Create subplot for hand distance\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(features_df['frame_idx'], features_df['hand_distance'], 'g-')\n",
    "    plt.title('Distance Between Hands')\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Processing\n",
    "\n",
    "Let's process multiple videos to get a more comprehensive view of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_multiple_videos(video_paths, max_videos=5):\n",
    "    \"\"\"Process multiple videos to extract features.\"\"\"\n",
    "    # Limit to max_videos\n",
    "    video_paths = video_paths[:max_videos]\n",
    "    \n",
    "    all_features = []\n",
    "    all_video_names = []\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = SignVideoProcessor()\n",
    "    \n",
    "    for path in tqdm(video_paths, desc=\"Processing videos\"):\n",
    "        video_name = os.path.basename(path)\n",
    "        \n",
    "        try:\n",
    "            # Process video\n",
    "            result = processor.process_video(path, max_frames=30)\n",
    "            \n",
    "            # Extract hand landmarks\n",
    "            if 'hand_landmarks' in result:\n",
    "                # Convert to list of dicts\n",
    "                landmarks = []\n",
    "                for i in range(len(result['hand_landmarks'])):\n",
    "                    landmarks.append({\n",
    "                        'left_hand': result['hand_landmarks'][i]['left'],\n",
    "                        'right_hand': result['hand_landmarks'][i]['right'],\n",
    "                        'pose': None  # No pose in this processor output\n",
    "                    })\n",
    "                \n",
    "                # Extract features\n",
    "                features = extract_hand_features(landmarks)\n",
    "                if features is not None:\n",
    "                    features['video_name'] = video_name\n",
    "                    all_features.append(features)\n",
    "                    all_video_names.append(video_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_name}: {e}\")\n",
    "    \n",
    "    # Concatenate features\n",
    "    if all_features:\n",
    "        combined_features = pd.concat(all_features, ignore_index=True)\n",
    "        return combined_features, all_video_names\n",
    "    else:\n",
    "        return None, all_video_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Process multiple videos\n",
    "# Get paths to a few videos\n",
    "sample_size = min(5, len(video_props_df))\n",
    "sample_videos = video_props_df.sample(sample_size, random_state=42)\n",
    "\n",
    "video_paths = []\n",
    "for idx, row in sample_videos.iterrows():\n",
    "    video_path = os.path.join(VIDEO_DIR, row['filename'])\n",
    "    \n",
    "    # Try common extensions if needed\n",
    "    if not os.path.exists(video_path):\n",
    "        for ext in [\".mp4\", \".avi\", \".mov\"]:\n",
    "            test_path = os.path.join(VIDEO_DIR, f\"{row['filename']}{ext}\")\n",
    "            if os.path.exists(test_path):\n",
    "                video_path = test_path\n",
    "                break\n",
    "    \n",
    "    if os.path.exists(video_path):\n",
    "        video_paths.append(video_path)\n",
    "\n",
    "if video_paths:\n",
    "    batch_features, video_names = process_multiple_videos(video_paths)\n",
    "    \n",
    "    if batch_features is not None:\n",
    "        print(f\"Processed {len(video_names)} videos\")\n",
    "        display(batch_features.head())\n",
    "        \n",
    "        # Visualize hand spread across videos\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for video_name in video_names:\n",
    "            video_data = batch_features[batch_features['video_name'] == video_name]\n",
    "            plt.plot(video_data['frame_idx'], video_data['left_hand_spread'], '-', label=f\"{video_name} (Left)\")\n",
    "        \n",
    "        plt.title('Left Hand Spread Comparison')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Spread')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No video paths found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "Based on our exploration, we can draw the following conclusions for the Sign2Text project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "1. **Video Characteristics**: Our dataset consists of videos with varying durations, resolutions, and frame rates. Most videos are [summary of analysis].\n",
    "\n",
    "2. **Hand Detection**: MediaPipe successfully detects hand landmarks in most frames, with [percentage]% of frames having at least one hand detected.\n",
    "\n",
    "3. **Feature Effectiveness**: The most promising features for sign recognition include:\n",
    "   - Hand trajectory (position over time)\n",
    "   - Hand spread (openness of the hand)\n",
    "   - Relative position between hands\n",
    "   - Hand velocity and acceleration\n",
    "\n",
    "4. **Data Quality**: Overall, the dataset appears [assessment of quality] for training a sign language recognition model. Some challenges include [list challenges if any]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Based on this exploration, we recommend the following next steps:\n",
    "\n",
    "1. **Data Processing Pipeline**:\n",
    "   - Implement consistent preprocessing to handle videos of different durations and resolutions\n",
    "   - Extract and normalize hand landmarks using MediaPipe\n",
    "   - Implement data augmentation for better model generalization\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - Create temporal features capturing hand movement patterns\n",
    "   - Include both raw landmarks and engineered features\n",
    "   - Normalize features to be invariant to person size and camera position\n",
    "\n",
    "3. **Model Architecture**:\n",
    "   - Use a hybrid CNN-LSTM or Transformer architecture\n",
    "   - Combine visual features from frames with landmark features\n",
    "   - Consider attention mechanisms to focus on important frames in the sequence\n",
    "\n",
    "4. **Evaluation Metrics**:\n",
    "   - Track both frame-level and sequence-level accuracy\n",
    "   - Use Word Error Rate (WER) for text output evaluation\n",
    "   - Consider user studies for real-world effectiveness"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
